{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!/usr/bin/env python\n",
    "coding: utf-8\n",
    "\n",
    "!/usr/bin/env python\n",
    "coding: utf-8\n",
    "\n",
    "\n",
    "## 1. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Prevent fork warnings with DataLoader\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import optuna\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} \"\n",
    "              f\"({torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB)\")\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class with all hyperparameters and settings.\"\"\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    model_name: str = \"answerdotai/ModernBERT-large\"\n",
    "    num_labels: int = 20\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    learning_rate: float = 3e-5\n",
    "    batch_size: int = 16  # Per-GPU batch size (total = batch_size \u00d7 num_gpus)\n",
    "    num_epochs: int = 4\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Layer Freezing\n",
    "    freeze_layers: bool = True\n",
    "    freeze_ratio: float = 0.5  # Freeze bottom 50% of encoder layers (14/28)\n",
    "    \n",
    "    # Data Configuration\n",
    "    dataset_name: str = \"SetFit/20_newsgroups\"\n",
    "    max_length: int = 256  \n",
    "    \n",
    "    # Training Settings\n",
    "    seed: int = 42\n",
    "    use_fp16: bool = True\n",
    "    save_model: bool = True\n",
    "    output_dir: str = \"./output\"\n",
    "    \n",
    "    # Device (auto-detected)\n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_gpus: int = field(default_factory=lambda: torch.cuda.device_count() if torch.cuda.is_available() else 0)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.device == \"cpu\":\n",
    "            self.use_fp16 = False\n",
    "        # Scale batch size across GPUs\n",
    "        self.total_batch_size = self.batch_size * max(1, self.num_gpus)\n",
    "            \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"num_labels\": self.num_labels,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size_per_gpu\": self.batch_size,\n",
    "            \"num_gpus\": self.num_gpus,\n",
    "            \"total_batch_size\": self.total_batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"warmup_ratio\": self.warmup_ratio,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"freeze_layers\": self.freeze_layers,\n",
    "            \"freeze_ratio\": self.freeze_ratio,\n",
    "            \"seed\": self.seed,\n",
    "            \"use_fp16\": self.use_fp16,\n",
    "            \"device\": self.device,\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Exploration & Statistical Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def explore_dataset(config):\n",
    "    \"\"\"Load and display comprehensive dataset statistics.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET EXPLORATION: 20 Newsgroups\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load raw dataset\n",
    "    dataset = load_dataset(config.dataset_name)\n",
    "    train_data = dataset['train']\n",
    "    test_data = dataset['test']\n",
    "    \n",
    "    label_names = [\n",
    "        'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    "        'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    "        'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    "        'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n",
    "        'sci.space', 'soc.religion.christian', 'talk.politics.guns',\n",
    "        'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n",
    "    ]\n",
    "    \n",
    "    # --- Basic Info ---\n",
    "    print(f\"\\n{'\u2500'*50}\")\n",
    "    print(f\"  Dataset: {config.dataset_name}\")\n",
    "    print(f\"  Number of classes: {len(label_names)}\")\n",
    "    print(f\"  Train samples: {len(train_data):,}\")\n",
    "    print(f\"  Test samples:  {len(test_data):,}\")\n",
    "    print(f\"  Total samples: {len(train_data) + len(test_data):,}\")\n",
    "    print(f\"  Features: {list(train_data.features.keys())}\")\n",
    "    print(f\"{'\u2500'*50}\")\n",
    "    \n",
    "    # --- Class Distribution ---\n",
    "    print(f\"\\n{'\u2500'*50}\")\n",
    "    print(\"  CLASS DISTRIBUTION\")\n",
    "    print(f\"{'\u2500'*50}\")\n",
    "    \n",
    "    train_labels = train_data['label']\n",
    "    test_labels = test_data['label']\n",
    "    train_counts = Counter(train_labels)\n",
    "    test_counts = Counter(test_labels)\n",
    "    \n",
    "    print(f\"\\n  {'Category':<35} {'Train':>6} {'Test':>6} {'Total':>6}\")\n",
    "    print(f\"  {'\u2500'*55}\")\n",
    "    for i, name in enumerate(label_names):\n",
    "        tr = train_counts.get(i, 0)\n",
    "        te = test_counts.get(i, 0)\n",
    "        bar = '\u2588' * (tr // 20)\n",
    "        print(f\"  {name:<35} {tr:>6} {te:>6} {tr+te:>6}  {bar}\")\n",
    "    \n",
    "    print(f\"  {'\u2500'*55}\")\n",
    "    print(f\"  {'TOTAL':<35} {len(train_data):>6} {len(test_data):>6} {len(train_data)+len(test_data):>6}\")\n",
    "    \n",
    "    # Class balance metrics\n",
    "    train_counts_list = [train_counts.get(i, 0) for i in range(len(label_names))]\n",
    "    print(f\"\\n  Train class balance:\")\n",
    "    print(f\"    Min samples/class: {min(train_counts_list)}\")\n",
    "    print(f\"    Max samples/class: {max(train_counts_list)}\")\n",
    "    print(f\"    Mean samples/class: {np.mean(train_counts_list):.1f}\")\n",
    "    print(f\"    Std samples/class: {np.std(train_counts_list):.1f}\")\n",
    "    print(f\"    Imbalance ratio (max/min): {max(train_counts_list)/max(min(train_counts_list),1):.2f}\")\n",
    "    \n",
    "    # --- Text Length Statistics ---\n",
    "    print(f\"\\n{'\u2500'*50}\")\n",
    "    print(\"  TEXT LENGTH STATISTICS (Training Set)\")\n",
    "    print(f\"{'\u2500'*50}\")\n",
    "    \n",
    "    texts = train_data['text']\n",
    "    char_lengths = [len(t) for t in texts]\n",
    "    word_lengths = [len(t.split()) for t in texts]\n",
    "    \n",
    "    for metric_name, lengths in [(\"Character lengths\", char_lengths), (\"Word counts\", word_lengths)]:\n",
    "        arr = np.array(lengths)\n",
    "        print(f\"\\n  {metric_name}:\")\n",
    "        print(f\"    Min:    {arr.min():>8,}\")\n",
    "        print(f\"    Max:    {arr.max():>8,}\")\n",
    "        print(f\"    Mean:   {arr.mean():>8,.1f}\")\n",
    "        print(f\"    Median: {np.median(arr):>8,.1f}\")\n",
    "        print(f\"    Std:    {arr.std():>8,.1f}\")\n",
    "        print(f\"    P25:    {np.percentile(arr, 25):>8,.1f}\")\n",
    "        print(f\"    P75:    {np.percentile(arr, 75):>8,.1f}\")\n",
    "        print(f\"    P95:    {np.percentile(arr, 95):>8,.1f}\")\n",
    "    \n",
    "    # Token-level stats with tokenizer\n",
    "    print(f\"\\n  Tokenized lengths (using {config.model_name} tokenizer):\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    # Sample for speed (full tokenization on large dataset is slow)\n",
    "    sample_size = min(2000, len(texts))\n",
    "    sample_texts = random.sample(texts, sample_size)\n",
    "    token_lengths = [len(tokenizer.encode(t)) for t in sample_texts]\n",
    "    arr = np.array(token_lengths)\n",
    "    \n",
    "    print(f\"    (Sampled {sample_size:,} documents)\")\n",
    "    print(f\"    Min:    {arr.min():>8,}\")\n",
    "    print(f\"    Max:    {arr.max():>8,}\")\n",
    "    print(f\"    Mean:   {arr.mean():>8,.1f}\")\n",
    "    print(f\"    Median: {np.median(arr):>8,.1f}\")\n",
    "    print(f\"    P95:    {np.percentile(arr, 95):>8,.1f}\")\n",
    "    \n",
    "    # Coverage at different max_length thresholds\n",
    "    print(f\"\\n  Token coverage at different max_length:\")\n",
    "    for ml in [128, 256, 512]:\n",
    "        coverage = (arr <= ml).sum() / len(arr) * 100\n",
    "        print(f\"    max_length={ml}: {coverage:.1f}% of documents fully covered\")\n",
    "    print(f\"    \u2192 Using max_length={config.max_length}\")\n",
    "    \n",
    "    # --- Sample Documents ---\n",
    "    print(f\"\\n{'\u2500'*50}\")\n",
    "    print(\"  SAMPLE DOCUMENTS (first 200 chars)\")\n",
    "    print(f\"{'\u2500'*50}\")\n",
    "    \n",
    "    # Show 1 sample per first 5 classes\n",
    "    for i in range(min(5, len(label_names))):\n",
    "        # Find first document with this label\n",
    "        for j, lbl in enumerate(train_labels):\n",
    "            if lbl == i:\n",
    "                text_preview = texts[j][:200].replace('\\n', ' ')\n",
    "                print(f\"\\n  [{label_names[i]}]\")\n",
    "                print(f\"  \\\"{text_preview}...\\\"\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n{'\u2500'*50}\")\n",
    "    print(f\"  (Showing 5 of {len(label_names)} classes)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Run dataset exploration\n",
    "dataset = explore_dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Data Loading and Preprocessing for 20 Newsgroups\n",
    "\n",
    "Design Decisions:\n",
    "-----------------\n",
    "1. Tokenization: ModernBERT-large tokenizer\n",
    "2. Padding: max_length for uniform batch shapes (better for DataParallel)\n",
    "3. DataLoader: 4 workers per GPU, pin memory\n",
    "\"\"\"\n",
    "\n",
    "def get_label_names() -> list:\n",
    "    \"\"\"Get the 20 newsgroup category names.\"\"\"\n",
    "    return [\n",
    "        'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    "        'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    "        'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    "        'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n",
    "        'sci.space', 'soc.religion.christian', 'talk.politics.guns',\n",
    "        'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_and_prepare_data(config, dataset=None) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Load 20 newsgroups dataset, tokenize, and create DataLoaders.\"\"\"\n",
    "    print(f\"\\nPreparing data for training...\")\n",
    "    \n",
    "    # Use pre-loaded dataset if available (from exploration step)\n",
    "    if dataset is None:\n",
    "        dataset = load_dataset(config.dataset_name)\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    test_split = dataset['test'].train_test_split(test_size=0.5, stratify_by_column=\"label\", seed=config.seed)\n",
    "    val_dataset = test_split['train']\n",
    "    test_dataset = test_split['test']\n",
    "    \n",
    "    print(f\"  Train size: {len(train_dataset)}\")\n",
    "    print(f\"  Validation size: {len(val_dataset)}\")\n",
    "    print(f\"  Test size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    print(\"  Tokenizing datasets...\")\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing train\")\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing val\")\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing test\")\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    columns = ['input_ids', 'attention_mask', 'label']\n",
    "    train_dataset.set_format(type='torch', columns=columns)\n",
    "    val_dataset.set_format(type='torch', columns=columns)\n",
    "    test_dataset.set_format(type='torch', columns=columns)\n",
    "    \n",
    "    # DataLoader config \u2014 use total_batch_size (accounts for multi-GPU)\n",
    "    num_workers = 4 if config.device == 'cuda' else 0\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.total_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if config.device == 'cuda' else False,\n",
    "        drop_last=True  # Avoids uneven batch splits across GPUs\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.total_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if config.device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.total_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if config.device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"  DataLoaders ready \u2014 Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Load data (reuses the dataset from exploration to avoid re-downloading)\n",
    "train_loader, val_loader, test_loader = load_and_prepare_data(config, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model(config):\n",
    "    \"\"\"Initialize ModernBERT-large with layer freezing and optional DataParallel.\"\"\"\n",
    "    print(f\"\\nLoading model: {config.model_name}\")\n",
    "    print(f\"  Number of classes: {config.num_labels}\")\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=config.num_labels,\n",
    "        finetuning_task=\"text-classification\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        config=model_config,\n",
    "        attn_implementation=\"eager\"  # Required: compiled attention breaks DataParallel\n",
    "    )\n",
    "    \n",
    "    # Layer freezing for efficiency\n",
    "    if config.freeze_layers:\n",
    "        # Freeze embeddings\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'embeddings'):\n",
    "            for param in model.model.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  \u2713 Froze embedding layer\")\n",
    "        elif hasattr(model, 'bert') and hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  \u2713 Froze embedding layer\")\n",
    "        \n",
    "        # Freeze bottom encoder layers\n",
    "        encoder_layers = None\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'encoder'):\n",
    "            encoder = model.model.encoder\n",
    "            if hasattr(encoder, 'layers'):\n",
    "                encoder_layers = encoder.layers\n",
    "            elif hasattr(encoder, 'layer'):\n",
    "                encoder_layers = encoder.layer\n",
    "        elif hasattr(model, 'bert') and hasattr(model.bert, 'encoder'):\n",
    "            encoder = model.bert.encoder\n",
    "            if hasattr(encoder, 'layer'):\n",
    "                encoder_layers = encoder.layer\n",
    "        \n",
    "        if encoder_layers is not None:\n",
    "            num_layers = len(encoder_layers)\n",
    "            num_freeze = int(num_layers * config.freeze_ratio)\n",
    "            for i, layer in enumerate(encoder_layers):\n",
    "                if i < num_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            print(f\"  \u2713 Froze {num_freeze}/{num_layers} encoder layers\")\n",
    "        else:\n",
    "            print(\"  \u26a0 Warning: Could not identify encoder layers for freezing\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"\\n  Parameter Summary:\")\n",
    "    print(f\"    Total:     {total_params:>12,}\")\n",
    "    print(f\"    Trainable: {trainable_params:>12,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    print(f\"    Frozen:    {frozen_params:>12,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "    \n",
    "    # Multi-GPU support with DataParallel\n",
    "    model.to(config.device)\n",
    "    if config.num_gpus > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"\\n  \u2713 DataParallel enabled across {config.num_gpus} GPUs\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, config, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = config.device\n",
    "\n",
    "        \n",
    "        # Access underlying model for parameter filtering (DataParallel wraps it)\n",
    "        base_model = model.module if hasattr(model, 'module') else model\n",
    "        \n",
    "        # Only optimize trainable parameters\n",
    "        self.optimizer = AdamW(\n",
    "            filter(lambda p: p.requires_grad, base_model.parameters()),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Total optimizer steps = one step per batch, per epoch\n",
    "        self.total_steps = len(train_loader) * config.num_epochs\n",
    "        self.warmup_steps = int(self.total_steps * config.warmup_ratio)\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.total_steps\n",
    "        )\n",
    "        \n",
    "        # Modern AMP API\n",
    "        self.scaler = GradScaler(\"cuda\") if config.use_fp16 else None\n",
    "        self.use_fp16 = config.use_fp16\n",
    "        \n",
    "        self.history = {'train_loss': [], 'learning_rate': []}\n",
    "        \n",
    "        print(f\"\\nTraining Configuration:\")\n",
    "        print(f\"  Device: {self.device} \u00d7 {config.num_gpus} GPUs\")\n",
    "        print(f\"  Total optimizer steps: {self.total_steps}\")\n",
    "        print(f\"  Warmup steps: {self.warmup_steps}\")\n",
    "        print(f\"  Mixed precision (FP16): {self.use_fp16}\")\n",
    "    \n",
    "    def _get_trainable_params(self):\n",
    "        \"\"\"Get trainable parameters from model (handles DataParallel).\"\"\"\n",
    "        base_model = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        return filter(lambda p: p.requires_grad, base_model.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_val(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        for batch in self.val_loader:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            if self.use_fp16:\n",
    "                with autocast(\"cuda\"):\n",
    "                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            else:\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss.mean() if outputs.loss.dim() > 0 else outputs.loss\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(self.val_loader)\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch, stepping the optimizer on every batch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            self.train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\",\n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.use_fp16:\n",
    "                with autocast(\"cuda\"):\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    # DataParallel returns averaged loss across GPUs\n",
    "                    loss = outputs.loss.mean()\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self._get_trainable_params(),\n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss.mean()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self._get_trainable_params(),\n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Starting Training\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            \n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            self.history['learning_rate'].append(current_lr)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f} - \"\n",
    "                  f\"LR: {current_lr:.2e} - \"\n",
    "                  f\"Time: {epoch_time:.1f}s\")\n",
    "            \n",
    "            # Memory report\n",
    "            if torch.cuda.is_available():\n",
    "                for i in range(config.num_gpus):\n",
    "                    allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                    reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                    print(f\"  GPU {i} memory: {allocated:.1f} GB allocated, {reserved:.1f} GB reserved\")\n",
    "        \n",
    "        val_loss = self.evaluate_val()\n",
    "        print(f\"\\nFinal Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining Complete! Total time: {total_time/60:.1f} minutes\")\n",
    "        \n",
    "        return val_loss, self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(config.seed + trial.number)\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n",
    "    wd = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "    wr = trial.suggest_float('warmup_ratio', 0.0, 0.2)\n",
    "    fr = trial.suggest_float('freeze_ratio', 0.0, 0.8)\n",
    "    \n",
    "    trial_config = copy.deepcopy(config)\n",
    "    trial_config.learning_rate = lr\n",
    "    trial_config.weight_decay = wd\n",
    "    trial_config.warmup_ratio = wr\n",
    "    trial_config.freeze_ratio = fr\n",
    "    \n",
    "    print(f\"\\n--- Starting Trial {trial.number} ---\")\n",
    "    print(f\"Params: lr={lr:.2e}, weight_decay={wd:.4f}, warmup_ratio={wr:.2f}, freeze_ratio={fr:.2f}\")\n",
    "    \n",
    "    trial_model = get_model(trial_config)\n",
    "    trainer = Trainer(trial_model, trial_config, train_loader, val_loader)\n",
    "    val_loss, history = trainer.train()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_dir = os.path.join(trial_config.output_dir, f\"trial_{trial.number}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    save_model = trial_model.module if hasattr(trial_model, 'module') else trial_model\n",
    "    save_model.save_pretrained(checkpoint_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(trial_config.model_name)\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    trial.set_user_attr('checkpoint_dir', checkpoint_dir)\n",
    "    return val_loss\n",
    "\n",
    "# Run QRS\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "sampler = optuna.samplers.QMCSampler(seed=config.seed)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler, study_name=\"modernbert_hpo\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING OPTUNA QUASI-RANDOM SEARCH\")\n",
    "print(\"=\"*70)\n",
    "study.optimize(objective, n_trials=16)\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    fig = optuna.visualization.matplotlib.plot_slice(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"qrs_hyperparameters_vs_loss.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    fig2 = optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"qrs_param_importances.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved Optuna visualizations to {config.output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate Optuna plots: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader, config):\n",
    "    \"\"\"Comprehensive evaluation on test set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating on Test Set\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(config.device)\n",
    "        attention_mask = batch['attention_mask'].to(config.device)\n",
    "        labels = batch['label'].to(config.device)\n",
    "        \n",
    "        if config.use_fp16:\n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "        \n",
    "        # Handle DataParallel loss\n",
    "        loss = outputs.loss.mean() if outputs.loss.dim() > 0 else outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='weighted'\n",
    "    )\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    label_names = get_label_names()\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_names, digits=4)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n[Overall Metrics]\")\n",
    "    print(f\"  Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"\\n[Macro Averages]\")\n",
    "    print(f\"  Precision: {precision_macro:.4f}\")\n",
    "    print(f\"  Recall: {recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"\\n[Weighted Averages]\")\n",
    "    print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"  Recall: {recall_weighted:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_weighted:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT (Per-Class)\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'test_loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'label_names': label_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "completed_trials.sort(key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "top_trials = completed_trials[:3]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION RESULTS - TOP 3 MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, t in enumerate(top_trials):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"--- Loading Top {idx+1} Model (Trial {t.number}) ---\")\n",
    "    print(f\"Params: {t.params}\")\n",
    "    print(f\"Validation Loss: {t.value:.4f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    ckpt_dir = t.user_attrs['checkpoint_dir']\n",
    "    \n",
    "    # Load and Evaluate\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(ckpt_dir)\n",
    "    best_model.to(config.device)\n",
    "    if config.num_gpus > 1:\n",
    "        best_model = nn.DataParallel(best_model)\n",
    "        \n",
    "    results = evaluate(best_model, test_loader, config)\n",
    "    \n",
    "    print(f\"\\n[Top {idx+1} Model Test Metrics]\")\n",
    "    print(f\"  Test Loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
    "    print(f\"  Macro F1: {results['f1_macro']:.4f}\")\n",
    "    \n",
    "    final_dir = os.path.join(config.output_dir, f\"top{idx+1}_model\")\n",
    "    if os.path.exists(final_dir):\n",
    "        import shutil\n",
    "        shutil.rmtree(final_dir)\n",
    "    os.rename(ckpt_dir, final_dir)\n",
    "    print(f\"Saved Top {idx+1} Model to {final_dir}\")\n",
    "    \n",
    "    metrics_path = os.path.join(final_dir, \"evaluation_metrics.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        # Avoid saving non-serializable objects (like numpy arrays from evaluation)\n",
    "        json_results = {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in results.items() if k not in ['confusion_matrix']}\n",
    "        json.dump(json_results, f, indent=2)\n",
    "\n",
    "    # Save trial info\n",
    "    with open(os.path.join(final_dir, \"trial_info.json\"), 'w') as f:\n",
    "        json.dump({\"trial_id\": t.number, \"params\": t.params, \"val_loss\": t.value}, f, indent=2)\n",
    "\n",
    "# Cleanup other trial weights to save space\n",
    "import shutil\n",
    "for t in study.trials:\n",
    "    if 'checkpoint_dir' in t.user_attrs:\n",
    "        ckpt_dir = t.user_attrs['checkpoint_dir']\n",
    "        if os.path.exists(ckpt_dir):\n",
    "            shutil.rmtree(ckpt_dir)\n",
    "\n",
    "# Keep the global 'model' pointing to the Best Model (Top 1) for Section 12 OOD evaluation\n",
    "best_ckpt_dir = os.path.join(config.output_dir, \"top1_model\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_ckpt_dir)\n",
    "model.to(config.device)\n",
    "if config.num_gpus > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Out-of-Distribution (OOD) Detection \u2014 \"Null / Other\" Class\n",
    "\n",
    "Strategy: Maximum Softmax Probability (MSP) with temperature scaling.\n",
    "\n",
    "A sample is labelled \"null/other\" when:\n",
    "    max( softmax( logits / T ) ) < tau\n",
    "\n",
    "  T (temperature): reshapes prob mass before scoring.\n",
    "                   T > 1 softens probs (often improves separation);\n",
    "                   T = 1 is the vanilla MSP baseline.\n",
    "\n",
    "  tau (threshold): directly controls the FP / FN trade-off:\n",
    "      increase tau -> stricter  -> fewer OOD accepted (fewer FP),\n",
    "                                   more ID rejected   (more  FN)\n",
    "      decrease tau -> looser   -> fewer ID rejected   (fewer FN),\n",
    "                                   more OOD accepted  (more  FP)\n",
    "\n",
    "In-distribution (ID) : 20 Newsgroups test set (same split as Section 9)\n",
    "Out-of-distribution  : AG News \u2014 4-class news topics, completely different domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score\n",
    "from scipy.special import softmax as scipy_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Core helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits_and_labels(mdl, loader, cfg, has_labels=True):\n",
    "    \"\"\"Run the model; return (logits ndarray shape (N,20), labels ndarray | None).\"\"\"\n",
    "    mdl.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    for batch in tqdm(loader, desc=\"  collecting\", leave=False):\n",
    "        ids  = batch[\"input_ids\"].to(cfg.device)\n",
    "        mask = batch[\"attention_mask\"].to(cfg.device)\n",
    "        out  = mdl(input_ids=ids, attention_mask=mask)\n",
    "        all_logits.append(out.logits.cpu().float().numpy())\n",
    "        if has_labels and \"label\" in batch:\n",
    "            all_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "    logits = np.concatenate(all_logits, axis=0)\n",
    "    labels = np.array(all_labels, dtype=int) if all_labels else None\n",
    "    return logits, labels\n",
    "\n",
    "\n",
    "def msp_scores(logits, temperature=1.0):\n",
    "    \"\"\"Maximum Softmax Probability: higher score -> more confident -> more likely ID.\"\"\"\n",
    "    probs = scipy_softmax(logits / max(temperature, 1e-6), axis=1)\n",
    "    return probs.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_ood_data(cfg, n_samples=2000, seed=42):\n",
    "    \"\"\"Sample n_samples texts from AG News (test split) and return a DataLoader.\"\"\"\n",
    "    print(\"  Loading OOD dataset: AG News ...\")\n",
    "    raw = load_dataset(\"ag_news\", split=\"test\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(raw), size=min(n_samples, len(raw)), replace=False).tolist()\n",
    "    raw = raw.select(idx)\n",
    "\n",
    "    _tok = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "    def _tokenize(examples):\n",
    "        return _tok(\n",
    "            examples[\"text\"],\n",
    "            truncation=True, padding=\"max_length\",\n",
    "            max_length=cfg.max_length, return_tensors=None,\n",
    "        )\n",
    "\n",
    "    tok_data = raw.map(_tokenize, batched=True, desc=\"  tokenising OOD\",\n",
    "                       remove_columns=raw.column_names)\n",
    "    tok_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    ood_loader = DataLoader(\n",
    "        tok_data,\n",
    "        batch_size=cfg.total_batch_size,\n",
    "        shuffle=False, num_workers=0,\n",
    "        pin_memory=(cfg.device == \"cuda\"),\n",
    "    )\n",
    "    print(f\"  OOD samples: {len(tok_data)} | batches: {len(ood_loader)}\")\n",
    "    return ood_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Evaluation at one temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_ood_detection(id_logits, id_true, ood_logits, temperature, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluate OOD detection performance using MSP at a given temperature T.\n",
    "\n",
    "    Metrics reported\n",
    "    ----------------\n",
    "    AUROC      : area under ROC (ID=1, OOD=0)\n",
    "    AP         : average precision\n",
    "    FPR@TPR95  : false-pos rate when 95% of ID samples are accepted\n",
    "    Table      : per-tau FPR / FNR / retained-ID-accuracy / % ID retained\n",
    "    Plots      : ROC curve, score distributions, FP-FN trade-off curve\n",
    "    \"\"\"\n",
    "    id_conf  = msp_scores(id_logits,  temperature)\n",
    "    ood_conf = msp_scores(ood_logits, temperature)\n",
    "\n",
    "    y_true  = np.concatenate([np.ones(len(id_conf)),  np.zeros(len(ood_conf))])\n",
    "    y_score = np.concatenate([id_conf,                ood_conf])\n",
    "\n",
    "    auroc = roc_auc_score(y_true, y_score)\n",
    "    ap    = average_precision_score(y_true, y_score)\n",
    "\n",
    "    fpr_arr, tpr_arr, _ = roc_curve(y_true, y_score)\n",
    "    idx95 = np.searchsorted(tpr_arr, 0.95)\n",
    "    fpr95 = float(fpr_arr[min(idx95, len(fpr_arr) - 1)])\n",
    "\n",
    "    print(f\"\\n  T={temperature:.2f}  |  \"\n",
    "          f\"AUROC={auroc:.4f}  AP={ap:.4f}  FPR@TPR95={fpr95:.4f}\")\n",
    "\n",
    "    # Threshold table\n",
    "    id_preds = id_logits.argmax(axis=1)\n",
    "    print(f\"\\n  {'tau':>6} | {'FPR':>8} | {'FNR':>8} | \"\n",
    "          f\"{'ID acc (retained)':>20} | {'% ID kept':>10}\")\n",
    "    print(f\"  {'\u2500' * 62}\")\n",
    "    for tau in np.arange(0.10, 1.0, 0.10):\n",
    "        fpr_val = float((ood_conf >= tau).mean())\n",
    "        fnr_val = float((id_conf  <  tau).mean())\n",
    "        mask    = id_conf >= tau\n",
    "        acc_val = (float((id_preds[mask] == id_true[mask]).mean())\n",
    "                   if mask.sum() > 0 else float(\"nan\"))\n",
    "        pct     = float(mask.mean()) * 100\n",
    "        print(f\"  {tau:6.2f} | {fpr_val:8.4f} | {fnr_val:8.4f} | \"\n",
    "              f\"{acc_val:>20.4f} | {pct:>9.1f}%\")\n",
    "\n",
    "    # Plots\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(\n",
    "        f\"OOD Detection (MSP) \u2014 Temperature T = {temperature:.2f}\", fontsize=13\n",
    "    )\n",
    "\n",
    "    # 1. ROC curve\n",
    "    ax = axes[0]\n",
    "    ax.plot(fpr_arr, tpr_arr, lw=2, color=\"#4C72B0\", label=f\"AUROC = {auroc:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"grey\", lw=1)\n",
    "    ax.axvline(fpr95, color=\"#DD8452\", linestyle=\":\", lw=1.5,\n",
    "               label=f\"FPR@TPR95 = {fpr95:.3f}\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "    # 2. Confidence score distributions\n",
    "    ax = axes[1]\n",
    "    ax.hist(ood_conf, bins=60, alpha=0.65, color=\"#DD8452\",\n",
    "            label=\"OOD (AG News)\", density=True)\n",
    "    ax.hist(id_conf,  bins=60, alpha=0.65, color=\"#4C72B0\",\n",
    "            label=\"ID (20 Newsgroups)\", density=True)\n",
    "    ax.set_xlabel(\"Max Softmax Probability\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(\"Confidence Score Distributions\")\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "    # 3. FP / FN trade-off vs threshold tau\n",
    "    tau_range = np.linspace(0, 1, 300)\n",
    "    fp_curve  = [(ood_conf >= t).mean() for t in tau_range]\n",
    "    fn_curve  = [(id_conf  <  t).mean() for t in tau_range]\n",
    "    ax = axes[2]\n",
    "    ax.plot(tau_range, fp_curve, lw=2, color=\"#DD8452\",\n",
    "            label=\"FPR \u2014 OOD wrongly accepted\")\n",
    "    ax.plot(tau_range, fn_curve, lw=2, color=\"#4C72B0\",\n",
    "            label=\"FNR \u2014 ID wrongly rejected\")\n",
    "    ax.set_xlabel(\"Threshold tau\")\n",
    "    ax.set_ylabel(\"Error Rate\")\n",
    "    ax.set_title(\"FP / FN Trade-off vs Threshold\")\n",
    "    ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    tag  = f\"T{int(temperature * 100):04d}\"\n",
    "    path = os.path.join(save_dir, f\"ood_detection_{tag}.png\")\n",
    "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"  Plot saved -> {path}\")\n",
    "\n",
    "    return dict(auroc=auroc, ap=ap, fpr95=fpr95,\n",
    "                id_conf=id_conf, ood_conf=ood_conf, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 12 \u2014 OOD DETECTION: NULL / OTHER CLASS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n[1/3] Collecting in-distribution logits (20 Newsgroups test set) ...\")\n",
    "id_logits_ood, id_true_ood = collect_logits_and_labels(\n",
    "    model, test_loader, config, has_labels=True\n",
    ")\n",
    "\n",
    "print(\"\\n[2/3] Loading & collecting OOD logits (AG News) ...\")\n",
    "ood_loader_sec12 = load_ood_data(config, n_samples=2000)\n",
    "ood_logits_ood, _ = collect_logits_and_labels(\n",
    "    model, ood_loader_sec12, config, has_labels=False\n",
    ")\n",
    "\n",
    "print(\"\\n[3/3] Evaluating across temperatures ...\")\n",
    "temperatures_ood = [0.5, 1.0, 2.0, 5.0]\n",
    "all_ood_results  = {}\n",
    "\n",
    "for T in temperatures_ood:\n",
    "    print(f\"\\n{'\u2500' * 58}\")\n",
    "    print(f\"  Temperature T = {T}\")\n",
    "    print(f\"{'\u2500' * 58}\")\n",
    "    all_ood_results[T] = evaluate_ood_detection(\n",
    "        id_logits_ood, id_true_ood, ood_logits_ood,\n",
    "        temperature=T, save_dir=config.output_dir,\n",
    "    )\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OOD DETECTION \u2014 TEMPERATURE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  {'T':>5} | {'AUROC':>7} | {'AP':>7} | {'FPR@TPR95':>10}\")\n",
    "print(f\"  {'\u2500' * 42}\")\n",
    "best_T_ood = max(all_ood_results, key=lambda t: all_ood_results[t][\"auroc\"])\n",
    "for T, r in sorted(all_ood_results.items()):\n",
    "    flag = \"  <- best AUROC\" if T == best_T_ood else \"\"\n",
    "    print(f\"  {T:5.2f} | {r['auroc']:7.4f} | {r['ap']:7.4f} | {r['fpr95']:10.4f}{flag}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OOD Detection complete. Plots saved to:\", config.output_dir)\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}