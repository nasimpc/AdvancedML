{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!/usr/bin/env python\n",
    "coding: utf-8\n",
    "\n",
    "# ModernBERT Fine-tuning for 20 Newsgroups Text Classification\n",
    "\n",
    "An implementation of ModernBERT fine-tuning for multi-class text classification\n",
    "using the 20 newsgroups dataset, optimized for Kaggle T4 GPU (16 GB VRAM).\n",
    "\n",
    "## 1. Design Decisions\n",
    "\n",
    "| Parameter | Value | Justification |\n",
    "|-----------|-------|---------------|\n",
    "| **Model** | answerdotai/ModernBERT-base | Modern encoder with RoPE, 8192 context, pre-trained on 2T tokens |\n",
    "| **Learning Rate** | 5e-5 | Slightly higher than classic BERT; ModernBERT benefits from it |\n",
    "| **Batch Size** | 32 | T4 16GB handles this with FP16 at seq len 256 |\n",
    "| **Gradient Accum** | 2 | Effective batch size 64 for better convergence |\n",
    "| **Epochs** | 3 | ModernBERT converges faster; avoids overfitting |\n",
    "| **Max Length** | 256 | Good coverage of newsgroup posts within T4 memory budget |\n",
    "| **Optimizer** | AdamW | Weight-decoupled Adam for transformers |\n",
    "| **Scheduler** | Linear warmup + decay | Prevents early instability |\n",
    "| **Layer Freezing** | Bottom 50% of encoder | Faster training, less memory, prevents overfitting |\n",
    "\n",
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # Enable cuDNN auto-tuner for T4 optimization\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "All hyperparameters are centralized here with documented justifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Configuration for ModernBERT Fine-tuning on 20 Newsgroups\n",
    "(Optimized for Kaggle T4 GPU \u2014 16 GB VRAM)\n",
    "\n",
    "Design Decisions:\n",
    "-----------------\n",
    "1. Model: answerdotai/ModernBERT-base\n",
    "   - Modern bidirectional encoder pre-trained on 2 trillion tokens\n",
    "   - Uses Rotary Positional Embeddings (RoPE) and alternating local-global attention\n",
    "   - Native 8192-token context window (we use 256 for efficiency)\n",
    "   \n",
    "2. Learning Rate: 5e-5\n",
    "   - ModernBERT benefits from slightly higher LR than classic BERT\n",
    "   - Combined with warmup to prevent early instability\n",
    "   \n",
    "3. Batch Size: 32 (effective 64 with gradient accumulation)\n",
    "   - T4 with 16 GB VRAM handles batch_size=32 at seq_len=256 with FP16\n",
    "   - Gradient accumulation steps=2 gives effective batch of 64\n",
    "   \n",
    "4. Epochs: 3\n",
    "   - ModernBERT converges faster than classic BERT\n",
    "   - Prevents overfitting on 11K training examples\n",
    "   \n",
    "5. Max Sequence Length: 256\n",
    "   - Better coverage of newsgroup posts than 128\n",
    "   - Still memory-efficient on T4 with FP16\n",
    "   \n",
    "6. Layer Freezing: Bottom 50% of encoder layers\n",
    "   - Freezes embeddings + first 11 of 22 layers\n",
    "   - Drastically reduces trainable params and memory\n",
    "   - Pre-trained features in lower layers transfer well\n",
    "\n",
    "7. Warmup Ratio: 0.1\n",
    "   - 10% of training steps for linear warmup\n",
    "   \n",
    "8. Weight Decay: 0.01\n",
    "   - Standard L2 regularization for transformers\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class with all hyperparameters and settings.\"\"\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    model_name: str = \"answerdotai/ModernBERT-base\"\n",
    "    num_labels: int = 20\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    learning_rate: float = 5e-5\n",
    "    batch_size: int = 32  # T4 16GB can handle this with FP16 @ seq_len=256\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    gradient_accumulation_steps: int = 2  # Effective batch size = 64\n",
    "    \n",
    "    # Layer Freezing\n",
    "    freeze_layers: bool = True\n",
    "    freeze_ratio: float = 0.5  # Freeze bottom 50% of encoder layers\n",
    "    \n",
    "    # Data Configuration\n",
    "    dataset_name: str = \"SetFit/20_newsgroups\"\n",
    "    max_length: int = 256  # Better coverage than 128; T4 handles it fine\n",
    "    \n",
    "    # Training Settings\n",
    "    seed: int = 42\n",
    "    use_fp16: bool = True\n",
    "    save_model: bool = True\n",
    "    output_dir: str = \"./output\"\n",
    "    \n",
    "    # Device (auto-detected)\n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.device == \"cpu\":\n",
    "            self.use_fp16 = False\n",
    "            \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"num_labels\": self.num_labels,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"effective_batch_size\": self.batch_size * self.gradient_accumulation_steps,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"warmup_ratio\": self.warmup_ratio,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"gradient_accumulation_steps\": self.gradient_accumulation_steps,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"freeze_layers\": self.freeze_layers,\n",
    "            \"freeze_ratio\": self.freeze_ratio,\n",
    "            \"seed\": self.seed,\n",
    "            \"use_fp16\": self.use_fp16,\n",
    "            \"device\": self.device,\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Load the 20 newsgroups dataset from HuggingFace and tokenize with ModernBERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Data Loading and Preprocessing for 20 Newsgroups\n",
    "\n",
    "Design Decisions:\n",
    "-----------------\n",
    "1. Dataset Source: SetFit/20_newsgroups from HuggingFace\n",
    "   - Pre-cleaned version with headers, signatures, and quotations removed\n",
    "   - Follows sklearn best practices for realistic training\n",
    "   \n",
    "2. Tokenization Strategy:\n",
    "   - Use ModernBERT tokenizer (handles [CLS]/[SEP] automatically)\n",
    "   - Truncate to max_length (256)\n",
    "   - Pad to max_length for batching efficiency\n",
    "   \n",
    "3. DataLoader:\n",
    "   - 4 workers (Kaggle T4 instances have 4 vCPUs)\n",
    "   - Pin memory for faster GPU transfers\n",
    "\"\"\"\n",
    "\n",
    "def get_label_names() -> list:\n",
    "    \"\"\"Get the 20 newsgroup category names.\"\"\"\n",
    "    return [\n",
    "        'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    "        'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    "        'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    "        'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n",
    "        'sci.space', 'soc.religion.christian', 'talk.politics.guns',\n",
    "        'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_and_prepare_data(config) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Load 20 newsgroups dataset, tokenize, and create DataLoaders.\"\"\"\n",
    "    print(f\"Loading dataset: {config.dataset_name}\")\n",
    "    \n",
    "    # Load raw dataset from HuggingFace\n",
    "    dataset = load_dataset(config.dataset_name)\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    print(f\"Train size: {len(train_dataset)}\")\n",
    "    print(f\"Test size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing train\")\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing test\")\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    columns = ['input_ids', 'attention_mask', 'label']\n",
    "    train_dataset.set_format(type='torch', columns=columns)\n",
    "    test_dataset.set_format(type='torch', columns=columns)\n",
    "    \n",
    "    # Create DataLoaders (4 workers for Kaggle T4 which has 4 vCPUs)\n",
    "    num_workers = 4 if config.device == 'cuda' else 0\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if config.device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if config.device == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Created DataLoaders - Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader = load_and_prepare_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n",
    "\n",
    "Initialize ModernBERT model with classification head and optional layer freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "ModernBERT Model for Text Classification\n",
    "\n",
    "Design Decisions:\n",
    "-----------------\n",
    "1. Architecture: ModernBERT + SequenceClassification head\n",
    "   - Pre-trained ModernBERT-base (149M params) + linear classification head\n",
    "   - Uses [CLS] token representation for classification\n",
    "   \n",
    "2. Layer Freezing:\n",
    "   - Freeze embeddings and bottom 50% of encoder layers\n",
    "   - Only fine-tune top layers + classification head\n",
    "   - Reduces trainable params from ~149M to ~75M\n",
    "   - Faster training, less memory, better regularization on small datasets\n",
    "   \n",
    "3. Initialization:\n",
    "   - Load pre-trained weights from HuggingFace Hub\n",
    "   - Classification head initialized randomly (will be trained)\n",
    "\"\"\"\n",
    "\n",
    "def get_model(config):\n",
    "    \"\"\"Initialize ModernBERT model for sequence classification with optional layer freezing.\"\"\"\n",
    "    print(f\"Loading model: {config.model_name}\")\n",
    "    print(f\"Number of classes: {config.num_labels}\")\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=config.num_labels,\n",
    "        finetuning_task=\"text-classification\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "    \n",
    "    # Layer freezing for efficiency on T4\n",
    "    if config.freeze_layers:\n",
    "        # Freeze embeddings\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'embeddings'):\n",
    "            for param in model.model.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  Froze embedding layer\")\n",
    "        elif hasattr(model, 'bert') and hasattr(model.bert, 'embeddings'):\n",
    "            for param in model.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  Froze embedding layer\")\n",
    "        \n",
    "        # Freeze bottom encoder layers\n",
    "        encoder_layers = None\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'encoder'):\n",
    "            encoder = model.model.encoder\n",
    "            if hasattr(encoder, 'layers'):\n",
    "                encoder_layers = encoder.layers\n",
    "            elif hasattr(encoder, 'layer'):\n",
    "                encoder_layers = encoder.layer\n",
    "        elif hasattr(model, 'bert') and hasattr(model.bert, 'encoder'):\n",
    "            encoder = model.bert.encoder\n",
    "            if hasattr(encoder, 'layer'):\n",
    "                encoder_layers = encoder.layer\n",
    "        \n",
    "        if encoder_layers is not None:\n",
    "            num_layers = len(encoder_layers)\n",
    "            num_freeze = int(num_layers * config.freeze_ratio)\n",
    "            for i, layer in enumerate(encoder_layers):\n",
    "                if i < num_freeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "            print(f\"  Froze {num_freeze}/{num_layers} encoder layers\")\n",
    "        else:\n",
    "            print(\"  Warning: Could not identify encoder layers for freezing\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    print(f\"Frozen parameters: {frozen_params:,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trainer\n",
    "\n",
    "Training loop with AdamW optimizer, warmup scheduler, gradient accumulation, and FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Training Module for ModernBERT Fine-tuning (T4 Optimized)\n",
    "\n",
    "Design Decisions:\n",
    "-----------------\n",
    "1. Optimizer: AdamW\n",
    "   - Weight-decoupled Adam (fixes L2 regularization in Adam)\n",
    "   - Standard choice for transformer fine-tuning\n",
    "   \n",
    "2. Scheduler: Linear warmup + linear decay\n",
    "   - Warmup prevents early instability with pre-trained models\n",
    "   - Linear decay gradually reduces learning rate\n",
    "   \n",
    "3. Mixed Precision (FP16):\n",
    "   - ~2x faster training on T4\n",
    "   - ~50% memory reduction for larger batches\n",
    "   - Uses modern torch.amp API (not deprecated torch.cuda.amp)\n",
    "   \n",
    "4. Gradient Accumulation:\n",
    "   - Steps=2 gives effective batch size of 64\n",
    "   - Better convergence without extra memory\n",
    "   \n",
    "5. Gradient Clipping:\n",
    "   - Max norm = 1.0 (standard for transformers)\n",
    "   - Prevents exploding gradients\n",
    "\"\"\"\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer class for ModernBERT fine-tuning on T4 GPU.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, train_loader):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.device = config.device\n",
    "        self.grad_accum_steps = config.gradient_accumulation_steps\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Only optimize trainable parameters\n",
    "        self.optimizer = AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Total optimizer steps accounts for gradient accumulation\n",
    "        self.total_steps = (len(train_loader) // self.grad_accum_steps) * config.num_epochs\n",
    "        self.warmup_steps = int(self.total_steps * config.warmup_ratio)\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.total_steps\n",
    "        )\n",
    "        \n",
    "        # Modern AMP API\n",
    "        self.scaler = GradScaler(\"cuda\") if config.use_fp16 else None\n",
    "        self.use_fp16 = config.use_fp16\n",
    "        \n",
    "        self.history = {'train_loss': [], 'learning_rate': []}\n",
    "        \n",
    "        print(f\"\\nTraining Configuration:\")\n",
    "        print(f\"  Device: {self.device}\")\n",
    "        print(f\"  Total optimizer steps: {self.total_steps}\")\n",
    "        print(f\"  Warmup steps: {self.warmup_steps}\")\n",
    "        print(f\"  Gradient accumulation steps: {self.grad_accum_steps}\")\n",
    "        print(f\"  Effective batch size: {config.batch_size * self.grad_accum_steps}\")\n",
    "        print(f\"  Mixed precision (FP16): {self.use_fp16}\")\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch with gradient accumulation.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            self.train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\",\n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            if self.use_fp16:\n",
    "                with autocast(\"cuda\"):\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    loss = outputs.loss / self.grad_accum_steps\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                if (step + 1) % self.grad_accum_steps == 0:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss / self.grad_accum_steps\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                if (step + 1) % self.grad_accum_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * self.grad_accum_steps  # Undo the division for logging\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item() * self.grad_accum_steps:.4f}',\n",
    "                'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "        \n",
    "        # Handle remaining gradients if batches not divisible by accum steps\n",
    "        remaining = len(self.train_loader) % self.grad_accum_steps\n",
    "        if remaining != 0:\n",
    "            if self.use_fp16:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "                    self.config.max_grad_norm\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Starting Training\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            \n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            self.history['learning_rate'].append(current_lr)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f} - \"\n",
    "                  f\"LR: {current_lr:.2e} - \"\n",
    "                  f\"Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining Complete! Total time: {total_time/60:.1f} minutes\")\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Initialize trainer and train\n",
    "trainer = Trainer(model, config, train_loader)\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Evaluation Module for ModernBERT Fine-tuning\n",
    "\n",
    "Metrics:\n",
    "- Accuracy: Overall correctness\n",
    "- Macro F1: Balanced performance across all classes\n",
    "- Per-class metrics: Identify weak categories\n",
    "- Confusion matrix: Reveals class confusion patterns\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader, config):\n",
    "    \"\"\"Comprehensive evaluation on test set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating on Test Set\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(config.device)\n",
    "        attention_mask = batch['attention_mask'].to(config.device)\n",
    "        labels = batch['label'].to(config.device)\n",
    "        \n",
    "        if config.use_fp16:\n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "        \n",
    "        total_loss += outputs.loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='weighted'\n",
    "    )\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    label_names = get_label_names()\n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_names, digits=4)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n[Overall Metrics]\")\n",
    "    print(f\"  Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"\\n[Macro Averages]\")\n",
    "    print(f\"  Precision: {precision_macro:.4f}\")\n",
    "    print(f\"  Recall: {recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"\\n[Weighted Averages]\")\n",
    "    print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"  Recall: {recall_weighted:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_weighted:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT (Per-Class)\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'test_loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'label_names': label_names\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate(model, test_loader, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Test Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Macro F1 Score: {results['f1_macro']:.4f}\")\n",
    "print(f\"  Weighted F1 Score: {results['f1_weighted']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if config.save_model:\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(config.output_dir, \"model\")\n",
    "    model.save_pretrained(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save tokenizer for easy reloading\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(f\"Tokenizer saved to: {model_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(config.output_dir, \"training_history.json\")\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    metrics = {\n",
    "        'model': config.model_name,\n",
    "        'test_loss': results['test_loss'],\n",
    "        'accuracy': results['accuracy'],\n",
    "        'precision_macro': results['precision_macro'],\n",
    "        'recall_macro': results['recall_macro'],\n",
    "        'f1_macro': results['f1_macro'],\n",
    "        'precision_weighted': results['precision_weighted'],\n",
    "        'recall_weighted': results['recall_weighted'],\n",
    "        'f1_weighted': results['f1_weighted'],\n",
    "    }\n",
    "    metrics_path = os.path.join(config.output_dir, \"evaluation_metrics.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Evaluation metrics saved to: {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}