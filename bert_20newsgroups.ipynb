{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!/usr/bin/env python\n",
    "coding: utf-8\n",
    "\n",
    "# ModernBERT Fine-tuning for 20 Newsgroups Text Classification\n",
    "\n",
    "Optimized for **Kaggle T4 x2** (2Ã— NVIDIA Tesla T4, 16GB VRAM each).\n",
    "Uses `DataParallel` to leverage both GPUs and\n",
    "T4-optimized settings (Tensor Cores, cuDNN, FP16).\n",
    "\n",
    "## 1. Design Decisions (ModernBERT Full Fine-tuning + T4 x2)\n",
    "\n",
    "| Parameter | Value | Justification |\n",
    "|-----------|-------|---------------|\n",
    "| **Model** | answerdotai/ModernBERT-base | 22-layer modernized BERT with RoPE, GeGLU, 8192 context |\n",
    "| **Multi-GPU** | DataParallel (2Ã— T4) | Simple multi-GPU, ~1.8Ã— speedup |\n",
    "| **Frozen Layers** | None (all 22 trainable) | Full fine-tuning for maximum accuracy |\n",
    "| **Learning Rate (encoder)** | 2e-5 | Lower LR prevents catastrophic forgetting with all layers trainable |\n",
    "| **Learning Rate (head)** | 1e-3 | 50Ã— encoder LR â€” head is randomly initialized, needs fast convergence |\n",
    "| **Batch Size** | 32 (16 per GPU) | Good balance of gradient quality and memory |\n",
    "| **Epochs** | 3 | Full fine-tuning converges faster; 3 epochs prevents overfitting |\n",
    "| **Max Length** | 512 | Captures more newsgroup post context; ModernBERT handles efficiently |\n",
    "| **Weight Decay** | 0.01 | Moderate regularization â€” not too aggressive for full fine-tuning |\n",
    "| **Warmup** | 10% of steps | Sufficient warmup for all-layers training stability |\n",
    "| **Optimizer** | AdamW | Weight-decoupled Adam for transformers |\n",
    "| **Scheduler** | Cosine warmup + decay | Smooth convergence, better than linear for ModernBERT |\n",
    "| **FP16** | Yes (Tensor Cores) | T4 has FP16 Tensor Cores â€” ~2Ã— throughput |\n",
    "\n",
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# T4 supports Triton (CUDA 7.5 >= 7.0), so torch.compile works!\n",
    "# Enable cuDNN auto-tuner for \n",
    "# optimal convolution algorithms on T4\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "All hyperparameters are centralized here, optimized for Kaggle T4 x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Configuration for ModernBERT Full Fine-tuning on 20 Newsgroups\n",
    "Optimized for ModernBERT-base + Kaggle T4 x2 (2Ã— NVIDIA Tesla T4, 16GB each)\n",
    "\n",
    "Full Fine-tuning Strategy (Maximum Accuracy):\n",
    "----------------------------------------------\n",
    "1. ALL layers trainable (no freezing)\n",
    "   - Full fine-tuning allows ModernBERT to fully adapt its representations\n",
    "   - With ~12k examples, overfitting is managed via low LR + weight decay + few epochs\n",
    "   \n",
    "2. Discriminative Learning Rates: 2e-5 (encoder), 1e-3 (head)\n",
    "   - Encoder LR of 2e-5 prevents catastrophic forgetting of pretrained knowledge\n",
    "   - Head LR of 1e-3 (50Ã—) allows the randomly initialized classifier to converge fast\n",
    "   \n",
    "3. Weight Decay: 0.01\n",
    "   - Moderate regularization â€” prevents overfitting without constraining the model\n",
    "   - Less aggressive than 0.1 which was over-regularizing with full fine-tuning\n",
    "   \n",
    "4. LR Schedule: Cosine with 10% warmup\n",
    "   - Cosine annealing provides smooth convergence\n",
    "   - 10% warmup stabilizes gradients when all layers are trainable\n",
    "   \n",
    "5. Batch Size: 32 total (16 per GPU)\n",
    "   - Good gradient quality per step\n",
    "   - Comfortably fits in T4 VRAM with FP16 (all layers need gradients)\n",
    "   \n",
    "6. Max Length: 512\n",
    "   - Newsgroup posts average ~300 tokens; 512 captures most content\n",
    "   - ModernBERT handles longer sequences efficiently via FlashAttention\n",
    "   \n",
    "7. Epochs: 3\n",
    "   - Full fine-tuning converges faster than partial fine-tuning\n",
    "   - 3 epochs prevents overfitting (all ~149M params are updating)\n",
    "   \n",
    "T4 x2 Hardware Optimizations:\n",
    "------------------------------\n",
    "1. FP16 Mixed Precision (T4 Tensor Cores â†’ ~2Ã— throughput)\n",
    "2. DataParallel across both T4s\n",
    "3. num_workers=4 (Kaggle CPU cores), pin_memory=True\n",
    "4. cuDNN auto-tune benchmark\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class with all hyperparameters and settings.\"\"\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    model_name: str = \"answerdotai/ModernBERT-base\"\n",
    "    num_labels: int = 20\n",
    "    \n",
    "    # Layer Freezing Configuration\n",
    "    num_layers_to_freeze: int = 0  # 0 = all layers trainable (full fine-tuning)\n",
    "    \n",
    "    # Training Hyperparameters â€” Full fine-tuning for maximum accuracy\n",
    "    learning_rate_encoder: float = 2e-5   # Low LR prevents catastrophic forgetting\n",
    "    learning_rate_head: float = 1e-3      # 50Ã— encoder LR for randomly initialized head\n",
    "    batch_size: int = 32  # 32 total â†’ 16 per GPU with DataParallel\n",
    "    num_epochs: int = 3   # Full fine-tuning converges fast; 3 epochs prevents overfitting\n",
    "    warmup_ratio: float = 0.1   # 10% warmup for stability with all layers trainable\n",
    "    weight_decay: float = 0.01  # Moderate regularization for full fine-tuning\n",
    "    label_smoothing: float = 0.1  # Prevents overconfident predictions on noisy labels\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data Configuration\n",
    "    dataset_name: str = \"SetFit/20_newsgroups\"\n",
    "    max_length: int = 512  # ModernBERT handles 512 efficiently; captures more post content\n",
    "    num_workers: int = 4   # Kaggle has 4 CPU cores\n",
    "    \n",
    "    # Training Settings\n",
    "    seed: int = 42\n",
    "    use_fp16: bool = True   # T4 Tensor Cores excel at FP16\n",
    "    use_multi_gpu: bool = True  # Enable DataParallel for 2Ã— T4\n",
    "    save_model: bool = True\n",
    "    output_dir: str = \"./output\"\n",
    "    \n",
    "    # Device (auto-detected)\n",
    "    device: str = field(default_factory=lambda: \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.device == \"cpu\":\n",
    "            self.use_fp16 = False\n",
    "            self.use_multi_gpu = False\n",
    "        # Only use multi-GPU if more than 1 GPU is available\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() < 2:\n",
    "            self.use_multi_gpu = False\n",
    "            \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"num_labels\": self.num_labels,\n",
    "            \"num_layers_to_freeze\": self.num_layers_to_freeze,\n",
    "            \"learning_rate_encoder\": self.learning_rate_encoder,\n",
    "            \"learning_rate_head\": self.learning_rate_head,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"warmup_ratio\": self.warmup_ratio,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"label_smoothing\": self.label_smoothing,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"num_workers\": self.num_workers,\n",
    "            \"seed\": self.seed,\n",
    "            \"use_fp16\": self.use_fp16,\n",
    "            \"use_multi_gpu\": self.use_multi_gpu,\n",
    "            \"device\": self.device,\n",
    "            \"num_gpus\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Load the 20 newsgroups dataset from HuggingFace and tokenize with ModernBERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Data Loading and Preprocessing for 20 Newsgroups\n",
    "\n",
    "T4 x2 Optimizations:\n",
    "---------------------\n",
    "1. num_workers=4: Kaggle provides 4 CPU cores â€” use all for data loading\n",
    "2. pin_memory=True: Enables fast CPUâ†’GPU transfers via page-locked memory\n",
    "3. prefetch_factor=2: Pre-load 2 batches per worker to keep GPUs fed\n",
    "4. persistent_workers=True: Avoid worker respawn overhead between epochs\n",
    "\"\"\"\n",
    "\n",
    "def get_label_names() -> list:\n",
    "    \"\"\"Get the 20 newsgroup category names.\"\"\"\n",
    "    return [\n",
    "        'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    "        'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    "        'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    "        'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med',\n",
    "        'sci.space', 'soc.religion.christian', 'talk.politics.guns',\n",
    "        'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_and_prepare_data(config) -> Tuple[DataLoader, DataLoader, list]:\n",
    "    \"\"\"Load 20 newsgroups dataset, tokenize with DYNAMIC padding, create DataLoaders.\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, test_loader, label_names\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {config.dataset_name}\")\n",
    "    \n",
    "    # Load raw dataset from HuggingFace\n",
    "    dataset = load_dataset(config.dataset_name)\n",
    "    \n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    # Extract label names directly from the dataset (not hardcoded!)\n",
    "    label_names = train_dataset.features['label'].names\n",
    "    \n",
    "    print(f\"Train size: {len(train_dataset)}\")\n",
    "    print(f\"Test size: {len(test_dataset)}\")\n",
    "    print(f\"Labels ({len(label_names)}): {label_names}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding=False,  # DYNAMIC padding via DataCollatorWithPadding\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing train\")\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True, desc=\"Tokenizing test\")\n",
    "    \n",
    "    # Keep only needed columns\n",
    "    keep_columns = ['input_ids', 'attention_mask', 'label']\n",
    "    remove_cols = [c for c in train_dataset.column_names if c not in keep_columns]\n",
    "    if remove_cols:\n",
    "        train_dataset = train_dataset.remove_columns(remove_cols)\n",
    "        test_dataset = test_dataset.remove_columns(remove_cols)\n",
    "    \n",
    "    # Dynamic padding collator â€” pads each batch to its max length\n",
    "    # instead of padding ALL sequences to 512. This is the #1 accuracy fix.\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    \n",
    "    # Create DataLoaders â€” optimized for T4 x2\n",
    "    use_cuda = config.device == 'cuda'\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=use_cuda,\n",
    "        prefetch_factor=2 if config.num_workers > 0 else None,\n",
    "        persistent_workers=True if config.num_workers > 0 else False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=use_cuda,\n",
    "        prefetch_factor=2 if config.num_workers > 0 else None,\n",
    "        persistent_workers=True if config.num_workers > 0 else False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(f\"Created DataLoaders - Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "    print(f\"  Batch size: {config.batch_size} (effective per-GPU: {config.batch_size // max(1, torch.cuda.device_count()) if use_cuda else config.batch_size})\")\n",
    "    print(f\"  Workers: {config.num_workers}, pin_memory: {use_cuda}\")\n",
    "    print(f\"  Padding: DYNAMIC (per-batch) via DataCollatorWithPadding\")\n",
    "    \n",
    "    return train_loader, test_loader, label_names\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader, label_names = load_and_prepare_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n",
    "\n",
    "Initialize ModernBERT model with classification head, apply layer freezing,\n",
    "and wrap in DataParallel for multi-GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "ModernBERT Model for Text Classification with Layer Freezing + Multi-GPU\n",
    "\n",
    "T4 x2 Optimizations:\n",
    "---------------------\n",
    "1. DataParallel wraps the model to split batches across both T4 GPUs\n",
    "2. torch.compile (optional) â€” T4 supports Triton for kernel fusion\n",
    "3. Layer freezing reduces memory footprint â†’ larger effective batch sizes\n",
    "\"\"\"\n",
    "\n",
    "def disable_modernbert_compiled_mlp(model: nn.Module) -> int:\n",
    "    \"\"\"Disable ModernBERT's internal compiled MLP to avoid Dynamo/FX conflicts.\n",
    "\n",
    "    Some ModernBERT variants wire an attribute like `compiled_mlp` that is a\n",
    "    Dynamo-optimized callable. If the overall model is later traced/compiled\n",
    "    (directly or indirectly), PyTorch can raise:\n",
    "    \"Detected that you are using FX to symbolically trace a dynamo-optimized function\".\n",
    "\n",
    "    This function forces the eager MLP path when possible.\n",
    "    \"\"\"\n",
    "\n",
    "    import inspect\n",
    "\n",
    "    patched = 0\n",
    "\n",
    "    for module in model.modules():\n",
    "        if not hasattr(module, \"compiled_mlp\"):\n",
    "            continue\n",
    "\n",
    "        compiled = getattr(module, \"compiled_mlp\")\n",
    "\n",
    "        # 1) If this is a torch.compile() OptimizedModule, unwrap to eager.\n",
    "        eager = getattr(compiled, \"_orig_mod\", None)\n",
    "        if eager is not None:\n",
    "            try:\n",
    "                setattr(module, \"compiled_mlp\", eager)\n",
    "                patched += 1\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 2) Prefer swapping to the eager MLP module if it exists.\n",
    "        if hasattr(module, \"mlp\") and callable(getattr(module, \"mlp\")):\n",
    "            try:\n",
    "                setattr(module, \"compiled_mlp\", getattr(module, \"mlp\"))\n",
    "                patched += 1\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 3) If it's a wrapped callable, unwrap it.\n",
    "        if callable(compiled):\n",
    "            try:\n",
    "                unwrapped = inspect.unwrap(compiled)\n",
    "                if unwrapped is not compiled:\n",
    "                    setattr(module, \"compiled_mlp\", unwrapped)\n",
    "                    patched += 1\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 4) Last resort: wrap callable to be Dynamo-disabled.\n",
    "        try:\n",
    "            import torch._dynamo  # type: ignore\n",
    "\n",
    "            if callable(compiled):\n",
    "                setattr(module, \"compiled_mlp\", torch._dynamo.disable(compiled))\n",
    "                patched += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return patched\n",
    "\n",
    "def get_model(config):\n",
    "    \"\"\"Initialize ModernBERT model with layer freezing and optional multi-GPU.\"\"\"\n",
    "    print(f\"Loading model: {config.model_name}\")\n",
    "    print(f\"Number of classes: {config.num_labels}\")\n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=config.num_labels,\n",
    "        finetuning_task=\"text-classification\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        config=model_config\n",
    "    )\n",
    "\n",
    "    # Avoid Dynamo/FX tracing conflicts seen with some ModernBERT builds.\n",
    "    patched = disable_modernbert_compiled_mlp(model)\n",
    "    if patched:\n",
    "        print(f\"Disabled internal compiled MLP in {patched} module(s)\")\n",
    "    \n",
    "    # =========================================================\n",
    "    # Layer Freezing / Full Fine-tuning Strategy\n",
    "    # =========================================================\n",
    "    # ModernBERT-base architecture:\n",
    "    #   - Embeddings layer\n",
    "    #   - 22 transformer layers (model.model.layers[0..21])\n",
    "    #   - Classification head (model.classifier or model.head)\n",
    "    #\n",
    "    # If num_layers_to_freeze == 0: ALL parameters are trainable\n",
    "    # Otherwise: freeze embeddings + first N encoder layers\n",
    "    # =========================================================\n",
    "    \n",
    "    num_total_layers = len(model.model.layers)\n",
    "    num_to_freeze = config.num_layers_to_freeze\n",
    "    num_to_train = num_total_layers - num_to_freeze\n",
    "    \n",
    "    if num_to_freeze == 0:\n",
    "        # â”€â”€ Full fine-tuning: all parameters trainable â”€â”€\n",
    "        print(f\"\\nFull Fine-tuning Mode:\")\n",
    "        print(f\"  All {num_total_layers} encoder layers + embeddings + head are trainable\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        # â”€â”€ Partial fine-tuning: freeze early layers â”€â”€\n",
    "        # Step 1: Freeze ALL parameters first\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Step 2: Unfreeze the last few encoder layers\n",
    "        print(f\"\\nLayer Freezing Configuration:\")\n",
    "        print(f\"  Total encoder layers: {num_total_layers}\")\n",
    "        print(f\"  Frozen layers: {num_to_freeze} (layers 0-{num_to_freeze - 1})\")\n",
    "        print(f\"  Trainable layers: {num_to_train} (layers {num_to_freeze}-{num_total_layers - 1})\")\n",
    "        \n",
    "        for i in range(num_to_freeze, num_total_layers):\n",
    "            for param in model.model.layers[i].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Step 3: Unfreeze the classification head\n",
    "        head_unfrozen = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(keyword in name for keyword in ['classifier', 'head', 'cls', 'score']):\n",
    "                param.requires_grad = True\n",
    "                head_unfrozen = True\n",
    "        \n",
    "        if not head_unfrozen:\n",
    "            print(\"WARNING: Could not identify classification head parameters to unfreeze!\")\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'layers' not in name and 'embeddings' not in name:\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Step 4: Also unfreeze the final layer norm if it exists\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'final_norm' in name or 'norm' in name.split('.')[-1]:\n",
    "                parts = name.split('.')\n",
    "                is_in_frozen_layer = False\n",
    "                for j, part in enumerate(parts):\n",
    "                    if part == 'layers' and j + 1 < len(parts):\n",
    "                        try:\n",
    "                            layer_idx = int(parts[j + 1])\n",
    "                            if layer_idx < num_to_freeze:\n",
    "                                is_in_frozen_layer = True\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                if not is_in_frozen_layer:\n",
    "                    param.requires_grad = True\n",
    "    \n",
    "    # Print parameter statistics\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"\\nParameter Statistics:\")\n",
    "    print(f\"  Total parameters:     {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    print(f\"  Frozen parameters:    {frozen_params:,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "    \n",
    "    # Print which parameter groups are trainable\n",
    "    print(f\"\\nTrainable parameter groups:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  âœ“ {name} [{param.numel():,} params]\")\n",
    "    \n",
    "    # =========================================================\n",
    "    # Multi-GPU: Wrap with DataParallel for 2Ã— T4\n",
    "    # =========================================================\n",
    "    if config.use_multi_gpu and torch.cuda.device_count() > 1:\n",
    "        print(f\"\\nðŸš€ Wrapping model in DataParallel across {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model.to(config.device)\n",
    "    \n",
    "    # Print GPU memory usage after loading\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            print(f\"  GPU {i} memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trainer\n",
    "\n",
    "Training loop with discriminative learning rates, AdamW optimizer,\n",
    "cosine LR schedule, FP16 with T4 Tensor Cores, and multi-GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Training Module for ModernBERT Fine-tuning on T4 x2\n",
    "\n",
    "T4 x2 Optimizations:\n",
    "---------------------\n",
    "1. FP16 with GradScaler â€” T4 Tensor Cores give ~2Ã— throughput\n",
    "2. DataParallel handles batch splitting across GPUs automatically\n",
    "3. Loss averaging across GPUs for DataParallel (mean of per-GPU losses)\n",
    "4. Gradient accumulation ready if you want even larger effective batch sizes\n",
    "5. CUDA event-based timing for accurate GPU measurements\n",
    "\"\"\"\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer class for ModernBERT fine-tuning with multi-GPU and FP16.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, train_loader):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.device = config.device\n",
    "        self.is_parallel = isinstance(model, nn.DataParallel)\n",
    "        \n",
    "        # Get the underlying model for parameter grouping\n",
    "        base_model = model.module if self.is_parallel else model\n",
    "        \n",
    "        # Create parameter groups with discriminative learning rates\n",
    "        encoder_params = []\n",
    "        head_params = []\n",
    "        \n",
    "        for name, param in base_model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if any(keyword in name for keyword in ['classifier', 'head', 'cls', 'score']):\n",
    "                head_params.append(param)\n",
    "            else:\n",
    "                encoder_params.append(param)\n",
    "        \n",
    "        self.optimizer = AdamW([\n",
    "            {\n",
    "                'params': encoder_params,\n",
    "                'lr': config.learning_rate_encoder,\n",
    "                'weight_decay': config.weight_decay\n",
    "            },\n",
    "            {\n",
    "                'params': head_params,\n",
    "                'lr': config.learning_rate_head,\n",
    "                'weight_decay': config.weight_decay  # Regularize the head too\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        self.total_steps = len(train_loader) * config.num_epochs\n",
    "        self.warmup_steps = int(self.total_steps * config.warmup_ratio)\n",
    "        \n",
    "        # Label smoothing loss â€” prevents overconfident predictions on noisy labels\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
    "        \n",
    "        self.scheduler = get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.total_steps\n",
    "        )\n",
    "        \n",
    "        self.scaler = GradScaler('cuda') if config.use_fp16 else None\n",
    "        self.use_fp16 = config.use_fp16\n",
    "        \n",
    "        self.history = {'train_loss': [], 'train_accuracy': [], 'learning_rate': []}\n",
    "        \n",
    "        num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "        print(f\"\\nTraining Configuration:\")\n",
    "        print(f\"  Device: {self.device} ({'DataParallel on ' + str(num_gpus) + ' GPUs' if self.is_parallel else 'single GPU'})\")\n",
    "        print(f\"  Total batch size: {config.batch_size}\")\n",
    "        print(f\"  Per-GPU batch size: {config.batch_size // num_gpus}\")\n",
    "        print(f\"  Total steps: {self.total_steps}\")\n",
    "        print(f\"  Warmup steps: {self.warmup_steps}\")\n",
    "        print(f\"  Encoder LR: {config.learning_rate_encoder}\")\n",
    "        print(f\"  Head LR: {config.learning_rate_head}\")\n",
    "        print(f\"  Encoder params: {sum(p.numel() for p in encoder_params):,}\")\n",
    "        print(f\"  Head params: {sum(p.numel() for p in head_params):,}\")\n",
    "        print(f\"  Mixed precision (FP16): {self.use_fp16}\")\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Some ModernBERT builds create/overwrite `compiled_mlp` lazily.\n",
    "        # Re-disable it once per epoch to avoid nested Dynamo/FX issues.\n",
    "        base_model = self.model.module if self.is_parallel else self.model\n",
    "        try:\n",
    "            disable_modernbert_compiled_mlp(base_model)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            self.train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\",\n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            self.optimizer.zero_grad(set_to_none=True)  # More memory efficient than zero_grad()\n",
    "            \n",
    "            if self.use_fp16:\n",
    "                with autocast('cuda'):\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                    )\n",
    "                    # Compute loss with label smoothing (cleaner than model's built-in)\n",
    "                    loss = self.criterion(outputs.logits, labels)\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.numel()\n",
    "                \n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                # Compute loss with label smoothing\n",
    "                loss = self.criterion(outputs.logits, labels)\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.numel()\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            running_acc = (correct / total) if total else 0.0\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{running_acc:.4f}',\n",
    "                'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "\n",
    "        epoch_loss = total_loss / num_batches\n",
    "        epoch_acc = (correct / total) if total else 0.0\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Starting Training\")\n",
    "        if self.is_parallel:\n",
    "            print(f\"  Using {torch.cuda.device_count()} GPUs via DataParallel\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            \n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            self.history['learning_rate'].append(current_lr)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            # Print GPU memory stats per epoch\n",
    "            mem_info = \"\"\n",
    "            if torch.cuda.is_available():\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                    peak = torch.cuda.max_memory_allocated(i) / 1024**3\n",
    "                    mem_info += f\" | GPU{i}: {allocated:.1f}GB (peak {peak:.1f}GB)\"\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs} - \"\n",
    "                f\"Train Loss: {train_loss:.4f} - \"\n",
    "                f\"Train Acc: {train_acc:.4f} ({train_acc*100:.2f}%) - \"\n",
    "                f\"LR: {current_lr:.2e} - \"\n",
    "                f\"Time: {epoch_time:.1f}s{mem_info}\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining Complete! Total time: {total_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Final memory summary\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\nGPU Memory Summary:\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                peak = torch.cuda.max_memory_allocated(i) / 1024**3\n",
    "                total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "                print(f\"  GPU {i}: Peak {peak:.2f} GB / {total:.1f} GB ({100*peak/total:.0f}% utilization)\")\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Initialize trainer and train\n",
    "trainer = Trainer(model, config, train_loader)\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set.\n",
    "\n",
    "**Important:** We unwrap DataParallel and run inference on a single GPU.\n",
    "DataParallel.replicate() creates fresh model copies per forward pass,\n",
    "re-introducing ModernBERT's compiled_mlp (torch.compile) attributes\n",
    "which conflict with FX tracing. Single-GPU eval avoids this entirely\n",
    "and is standard practice (DataParallel mainly benefits training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Evaluation Module for ModernBERT Fine-tuning\n",
    "\n",
    "Metrics:\n",
    "- Accuracy: Overall correctness\n",
    "- Macro F1: Balanced performance across all classes\n",
    "- Per-class metrics: Identify weak categories\n",
    "- Confusion matrix: Reveals class confusion patterns\n",
    "\n",
    "Note: Evaluation runs on a single GPU (unwrapped from DataParallel)\n",
    "to avoid Dynamo/FX tracing conflicts with ModernBERT's compiled MLPs.\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader, config):\n",
    "    \"\"\"Comprehensive evaluation on test set (single-GPU, Dynamo-safe).\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating on Test Set\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # â”€â”€â”€ Step 1: Unwrap DataParallel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # DataParallel.replicate() deep-copies the model per GPU on every\n",
    "    # forward pass, re-creating compiled_mlp attrs we tried to remove.\n",
    "    # Solution: just use the base model on GPU 0 for inference.\n",
    "    is_parallel = isinstance(model, nn.DataParallel)\n",
    "    eval_model = model.module if is_parallel else model\n",
    "    \n",
    "    if is_parallel:\n",
    "        print(\"  Unwrapped DataParallel â†’ evaluating on single GPU\")\n",
    "    \n",
    "    # â”€â”€â”€ Step 2: Fully disable torch.compile / Dynamo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Reset Dynamo state and disable compiled MLPs on the base model\n",
    "    # to prevent any FX-vs-Dynamo conflicts during inference.\n",
    "    try:\n",
    "        import torch._dynamo\n",
    "        torch._dynamo.reset()\n",
    "        # Suppress any remaining Dynamo errors as fallback\n",
    "        torch._dynamo.config.suppress_errors = True\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    patched = disable_modernbert_compiled_mlp(eval_model)\n",
    "    if patched:\n",
    "        print(f\"  Disabled compiled MLP in {patched} module(s)\")\n",
    "    \n",
    "    # â”€â”€â”€ Step 3: Set eval mode and device â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    eval_model.eval()\n",
    "    eval_model.to(config.device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    # â”€â”€â”€ Step 4: Inference loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(config.device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(config.device, non_blocking=True)\n",
    "        labels = batch['labels'].to(config.device, non_blocking=True)\n",
    "        \n",
    "        with autocast('cuda', enabled=config.use_fp16):\n",
    "            outputs = eval_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "        \n",
    "        total_loss += outputs.loss.item()\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # â”€â”€â”€ Step 5: Calculate metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='weighted'\n",
    "    )\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    report = classification_report(all_labels, all_predictions, target_names=label_names, digits=4)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # â”€â”€â”€ Step 6: Print results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n[Overall Metrics]\")\n",
    "    print(f\"  Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"\\n[Macro Averages]\")\n",
    "    print(f\"  Precision: {precision_macro:.4f}\")\n",
    "    print(f\"  Recall: {recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"\\n[Weighted Averages]\")\n",
    "    print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"  Recall: {recall_weighted:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_weighted:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT (Per-Class)\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'test_loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'label_names': label_names\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate(model, test_loader, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  GPUs: {torch.cuda.device_count() if torch.cuda.is_available() else 0}Ã— {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"  Frozen layers: {config.num_layers_to_freeze} / 22\")\n",
    "print(f\"  Trainable layers: {22 - config.num_layers_to_freeze} + classification head\")\n",
    "print(f\"  Batch size: {config.batch_size} total\")\n",
    "print(f\"  Test Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Macro F1 Score: {results['f1_macro']:.4f}\")\n",
    "print(f\"  Weighted F1 Score: {results['f1_weighted']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if config.save_model:\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model â€” unwrap DataParallel if needed\n",
    "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    model_path = os.path.join(config.output_dir, \"model\")\n",
    "    base_model.save_pretrained(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save tokenizer for easy reloading\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(f\"Tokenizer saved to: {model_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(config.output_dir, \"training_history.json\")\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"Training history saved to: {history_path}\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    metrics = {\n",
    "        'model_name': config.model_name,\n",
    "        'num_layers_frozen': config.num_layers_to_freeze,\n",
    "        'num_gpus': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        'batch_size': config.batch_size,\n",
    "        'test_loss': results['test_loss'],\n",
    "        'accuracy': results['accuracy'],\n",
    "        'precision_macro': results['precision_macro'],\n",
    "        'recall_macro': results['recall_macro'],\n",
    "        'f1_macro': results['f1_macro'],\n",
    "        'precision_weighted': results['precision_weighted'],\n",
    "        'recall_weighted': results['recall_weighted'],\n",
    "        'f1_weighted': results['f1_weighted'],\n",
    "    }\n",
    "    metrics_path = os.path.join(config.output_dir, \"evaluation_metrics.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Evaluation metrics saved to: {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
